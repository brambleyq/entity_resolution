# ENTITY RESOLUTION

## What is Entity Resolution?
    Entity resolution is the process of matching entitities from two seperate sets of data that might have different formats, nameing conventions, or typos. These entities can be people in two seperate phone books, connecting nobel prize winners and patent holders, or a cell service providers customerbase with an electronics stores customers. 

## Matching Data Together
    To combine these data sets you can match overlapping data that is exactly the same so if a username is "William Smith" in both phone books they will be the same entry in the combined data but usernames "Bill Smith" and "William Smith" will be different entries in the combined data. This leads to sepreate entities with data in common and the same entites with slightly different data being entered as different in the new data set. These mistakes will happen unless both data sets use the exact same formating and contain no typos. To stop two entities from combining in the combined data set what could be done is use more overlapping data. Instead of using just username one could use phone number and address along side it. This makes it so it is less likely that an entity will share all this data but it still does not address the problem of the same entity being labeled as different when data is slightly different. To address this instead of looking for overlapping data that is exactly the same one can ask if the data is close to each other. Then ask how far apart are these two peices of data and find how close they need to be to be considered the same. 

## How Close is Close Enough?    
    Now it is possible to set all these parameters manually like "if the address is within .01 mile, the first name is within 1 letter, and the phone number is within 3 digits they are the same" and other simmilar questions about how close parameters should be. The problem is that now there is alot of manual work to be done. To figure out what kind of parameters are required for correct entity resolution it takes alot of fine tuning. Even after tuning the parameters for known data it is unknown if this will even be able to work for new data. The way to get past this problem is to make an algorithm to find these parameters automatically. The model to do this is XGBoost classifier as it can be trained on few parameters. 

## Creating data for the model
    The data used is patent data from the US patent and trademark office and npi data from the Centers for Medicare and Medicaid Services. This would mean that the model is matching doctors with patentees by their forename, surname, and address. To create data for the XGBoost model samples are taken from the data of both tables. Entries are placed with themselves and with slight typos of themselves and then labeled with a sucessful match. Entries are also mismatched and labeled as such these entries' features are then found. The features for this model are the levenshtein distances found using fuzzywuzzy.
    
## Training the Model
    To train the model it seems like every patentee will need to be compared with every doctor. This is a problem as now the work needed to be done is O(n^2). To fix this what needs to be done is to pair entries that are similar before doing a distance calculation. Though this is just a smaller version of entity resolution it can be done through fasttext. A vector is made for each of the forenames and then the closest vectors are paired with eachtother making the work for matching and findind distances O(n). Now that the nearest forenames are paired together along with the other parameters a distance feature is found for each parameter. This can now be fed into the model and all correct matches are found.